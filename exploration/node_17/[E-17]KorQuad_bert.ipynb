{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reliable-clinic",
   "metadata": {},
   "source": [
    "## 사전 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flying-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 폰트 설치를 확인합니다.  \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threatened-flour",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "positive-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_json_tree(data, indent=\"\"):\n",
    "    for key, value in data.items():\n",
    "        if type(value) == list:     # list 형태의 item은 첫번째 item만 출력\n",
    "            print(f'{indent}- {key}: [{len(value)}]')\n",
    "            print_json_tree(value[0], indent + \"  \")\n",
    "        else:\n",
    "            print(f'{indent}- {key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "christian-fighter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- version: KorQuAD_v1.0_train\n",
      "- data: [1420]\n",
      "  - paragraphs: [3]\n",
      "    - qas: [8]\n",
      "      - answers: [1]\n",
      "        - text: 교향곡\n",
      "        - answer_start: 54\n",
      "      - id: 6566495-0-0\n",
      "      - question: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "    - context: 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "  - title: 파우스트_서곡\n"
     ]
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_qna/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_qna/models'\n",
    "\n",
    "# 훈련데이터 확인\n",
    "train_json_path = data_dir + '/KorQuAD_v1.0_train.json'\n",
    "with open(train_json_path) as f:\n",
    "    train_json = json.load(f)\n",
    "    print_json_tree(train_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "steady-dodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- version: KorQuAD_v1.0_dev\n",
      "- data: [140]\n",
      "  - paragraphs: [2]\n",
      "    - qas: [7]\n",
      "      - answers: [1]\n",
      "        - text: 1989년 2월 15일\n",
      "        - answer_start: 0\n",
      "      - id: 6548850-0-0\n",
      "      - question: 임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?\n",
      "    - context: 1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의(폭력행위등처벌에관한법률위반)으로 지명수배되었다. 1989년 3월 12일 서울지방검찰청 공안부는 임종석의 사전구속영장을 발부받았다. 같은 해 6월 30일 평양축전에 임수경을 대표로 파견하여 국가보안법위반 혐의가 추가되었다. 경찰은 12월 18일~20일 사이 서울 경희대학교에서 임종석이 성명 발표를 추진하고 있다는 첩보를 입수했고, 12월 18일 오전 7시 40분 경 가스총과 전자봉으로 무장한 특공조 및 대공과 직원 12명 등 22명의 사복 경찰을 승용차 8대에 나누어 경희대학교에 투입했다. 1989년 12월 18일 오전 8시 15분 경 서울청량리경찰서는 호위 학생 5명과 함께 경희대학교 학생회관 건물 계단을 내려오는 임종석을 발견, 검거해 구속을 집행했다. 임종석은 청량리경찰서에서 약 1시간 동안 조사를 받은 뒤 오전 9시 50분 경 서울 장안동의 서울지방경찰청 공안분실로 인계되었다.\n",
      "  - title: 임종석\n"
     ]
    }
   ],
   "source": [
    "# 검증데이터 확인\n",
    "dev_json_path = data_dir + '/KorQuAD_v1.0_dev.json'\n",
    "with open(dev_json_path) as f:\n",
    "    dev_json = json.load(f)\n",
    "    print_json_tree(dev_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sudden-custody",
   "metadata": {},
   "source": [
    "# 1. KorQuAD 데이터셋 전처리 (1) 띄어쓰기 단위 정보관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "robust-resource",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "addressed-gambling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('파우스트', '파우스트')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whitespace가 2개인 경우를 처리해야 함\n",
    "\n",
    "string1 = '1839년 파우스트를 읽었다.'\n",
    "string2 = '1839년  파우스트를 읽었다.'\n",
    "string1[6:10], string2[7:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cultural-radius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' : ['1'] : [0]\n",
      "'8' : ['18'] : [0, 0]\n",
      "'3' : ['183'] : [0, 0, 0]\n",
      "'9' : ['1839'] : [0, 0, 0, 0]\n",
      "'년' : ['1839년'] : [0, 0, 0, 0, 0]\n",
      "' ' : ['1839년'] : [0, 0, 0, 0, 0, 0]\n",
      "'파' : ['1839년', '파'] : [0, 0, 0, 0, 0, 0, 1]\n",
      "'우' : ['1839년', '파우'] : [0, 0, 0, 0, 0, 0, 1, 1]\n",
      "'스' : ['1839년', '파우스'] : [0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "'트' : ['1839년', '파우스트'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "'를' : ['1839년', '파우스트를'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "' ' : ['1839년', '파우스트를'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "'읽' : ['1839년', '파우스트를', '읽'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2]\n",
      "'었' : ['1839년', '파우스트를', '읽었'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2]\n",
      "'다' : ['1839년', '파우스트를', '읽었다'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2]\n",
      "'.' : ['1839년', '파우스트를', '읽었다.'] : [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = []\n",
    "char_to_word = []\n",
    "prev_is_whitespace = True\n",
    "\n",
    "# 첫번째 문장(string1)에 대해 띄어쓰기 영역 정보를 표시\n",
    "for c in string1:\n",
    "    if _is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            word_tokens.append(c)\n",
    "        else:\n",
    "            word_tokens[-1] += c\n",
    "        prev_is_whitespace = False    \n",
    "    char_to_word.append(len(word_tokens) - 1)\n",
    "    print(f'\\'{c}\\' : {word_tokens} : {char_to_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fantastic-canada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' : ['1'] : [0]\n",
      "'8' : ['18'] : [0, 0]\n",
      "'3' : ['183'] : [0, 0, 0]\n",
      "'9' : ['1839'] : [0, 0, 0, 0]\n",
      "'년' : ['1839년'] : [0, 0, 0, 0, 0]\n",
      "' ' : ['1839년'] : [0, 0, 0, 0, 0, 0]\n",
      "' ' : ['1839년'] : [0, 0, 0, 0, 0, 0, 0]\n",
      "'파' : ['1839년', '파'] : [0, 0, 0, 0, 0, 0, 0, 1]\n",
      "'우' : ['1839년', '파우'] : [0, 0, 0, 0, 0, 0, 0, 1, 1]\n",
      "'스' : ['1839년', '파우스'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1]\n",
      "'트' : ['1839년', '파우스트'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]\n",
      "'를' : ['1839년', '파우스트를'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "' ' : ['1839년', '파우스트를'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
      "'읽' : ['1839년', '파우스트를', '읽'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2]\n",
      "'었' : ['1839년', '파우스트를', '읽었'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2]\n",
      "'다' : ['1839년', '파우스트를', '읽었다'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2]\n",
      "'.' : ['1839년', '파우스트를', '읽었다.'] : [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = []\n",
    "char_to_word = []\n",
    "prev_is_whitespace = True\n",
    "\n",
    "# 두번째 문장(string2)에 대해 띄어쓰기 영역 정보를 표시\n",
    "for c in string2:\n",
    "    if _is_whitespace(c):\n",
    "        prev_is_whitespace = True\n",
    "    else:\n",
    "        if prev_is_whitespace:\n",
    "            word_tokens.append(c)\n",
    "        else:\n",
    "            word_tokens[-1] += c\n",
    "        prev_is_whitespace = False    \n",
    "    char_to_word.append(len(word_tokens) - 1)\n",
    "    print(f'\\'{c}\\' : {word_tokens} : {char_to_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "gross-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_whitespace(string):\n",
    "    word_tokens = []\n",
    "    char_to_word = []\n",
    "    prev_is_whitespace = True\n",
    "\n",
    "    for c in string:\n",
    "        if _is_whitespace(c):\n",
    "            prev_is_whitespace = True\n",
    "        else:\n",
    "            if prev_is_whitespace:\n",
    "                word_tokens.append(c)\n",
    "            else:\n",
    "                word_tokens[-1] += c\n",
    "            prev_is_whitespace = False    \n",
    "        char_to_word.append(len(word_tokens) - 1)\n",
    "    \n",
    "    return word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dominant-board",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' : 0\n",
      "'8' : 0\n",
      "'3' : 0\n",
      "'9' : 0\n",
      "'년' : 0\n",
      "' ' : 0\n",
      "'파' : 1\n",
      "'우' : 1\n",
      "'스' : 1\n",
      "'트' : 1\n",
      "'를' : 1\n",
      "' ' : 1\n",
      "'읽' : 2\n",
      "'었' : 2\n",
      "'다' : 2\n",
      "'.' : 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['1839년', '파우스트를', '읽었다.'], [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫번째 문장(string1)에 대해 띄어쓰기 영역 정보를 표시\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string1)\n",
    "for c, i in zip(list(string1), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "colored-gambling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1' : 0\n",
      "'8' : 0\n",
      "'3' : 0\n",
      "'9' : 0\n",
      "'년' : 0\n",
      "' ' : 0\n",
      "' ' : 0\n",
      "'파' : 1\n",
      "'우' : 1\n",
      "'스' : 1\n",
      "'트' : 1\n",
      "'를' : 1\n",
      "' ' : 1\n",
      "'읽' : 2\n",
      "'었' : 2\n",
      "'다' : 2\n",
      "'.' : 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['1839년', '파우스트를', '읽었다.'],\n",
       " [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 두번째 문장(string2)에 대해 띄어쓰기 영역 정보를 표시\n",
    "word_tokens, char_to_word = _tokenize_whitespace(string2)\n",
    "for c, i in zip(list(string2), char_to_word):\n",
    "    print(f'\\'{c}\\' : {i}')\n",
    "\n",
    "word_tokens, char_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-packing",
   "metadata": {},
   "source": [
    "# KorQuAD 데이터셋 전처리 (2) Tokenize by Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "renewable-tourist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['▁1839', '년', '▁', '파우스트', '를', '▁읽', '었다', '.'], [0, 2, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")\n",
    "\n",
    "# word를 subword로 변경하면서 index 저장\n",
    "word_to_token = []\n",
    "context_tokens = []\n",
    "for (i, word) in enumerate(word_tokens):\n",
    "    word_to_token.append(len(context_tokens))\n",
    "    tokens = vocab.encode_as_pieces(word)  # SentencePiece를 사용해 Subword로 쪼갭니다.\n",
    "    for token in tokens:\n",
    "        context_tokens.append(token)\n",
    "\n",
    "context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "chief-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_vocab(vocab, context_words):\n",
    "    word_to_token = []\n",
    "    context_tokens = []\n",
    "    for (i, word) in enumerate(context_words):\n",
    "        word_to_token.append(len(context_tokens))\n",
    "        tokens = vocab.encode_as_pieces(word)\n",
    "        for token in tokens:\n",
    "            context_tokens.append(token)\n",
    "    return context_tokens, word_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "mechanical-tiffany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1839년', '파우스트를', '읽었다.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['▁1839', '년', '▁', '파우스트', '를', '▁읽', '었다', '.'], [0, 2, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(word_tokens)  # 처리해야 할 word 단위 입력\n",
    "\n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "context_tokens, word_to_token   # Subword 단위로 토큰화한 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-guitar",
   "metadata": {},
   "source": [
    "# KorQuAD 데이터셋 전처리 (3) Improve Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "later-quarterly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[context]  1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "[question]  바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "[answer]  교향곡\n",
      "[answer_start] index:  54 character:  교\n",
      "[answer_end]index:  56 character:  곡\n"
     ]
    }
   ],
   "source": [
    "context = train_json['data'][0]['paragraphs'][0]['context']\n",
    "question = train_json['data'][0]['paragraphs'][0]['qas'][0]['question']\n",
    "answer_text = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['text']\n",
    "answer_start = train_json['data'][0]['paragraphs'][0]['qas'][0]['answers'][0]['answer_start']\n",
    "answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "print('[context] ', context)\n",
    "print('[question] ', question)\n",
    "print('[answer] ', answer_text)\n",
    "print('[answer_start] index: ', answer_start, 'character: ', context[answer_start])\n",
    "print('[answer_end]index: ', answer_end, 'character: ', context[answer_end])\n",
    "\n",
    "# answer_text에 해당하는 context 영역을 정확히 찾아내야 합니다. \n",
    "assert context[answer_start:answer_end + 1] == answer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "human-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1839년', '바그너는', '괴테의', '파우스트을', '처음', '읽고', '그', '내용에', '마음이', '끌려', '이를', '소재로', '해서', '하나의', '교향곡을', '쓰려는', '뜻을', '갖는다.', '이', '시기']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3],\n",
       " '1839년 바그너는 괴테의 파우스트을')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# context를 띄어쓰기(word) 단위로 토큰화한 결과를 살펴봅니다. \n",
    "word_tokens, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "print( word_tokens[:20])\n",
    "\n",
    "char_to_word[:20], context[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hybrid-joyce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['▁1839', '년']\n",
      "2 ['▁바그너', '는']\n",
      "4 ['▁괴테', '의']\n",
      "6 ['▁', '파우스트', '을']\n",
      "9 ['▁처음']\n",
      "10 ['▁읽고']\n",
      "11 ['▁그']\n",
      "12 ['▁내용에']\n",
      "13 ['▁마음이']\n",
      "14 ['▁끌려']\n",
      "15 ['▁이를']\n",
      "16 ['▁소재로']\n",
      "17 ['▁해서']\n",
      "18 ['▁하나의']\n",
      "19 ['▁교향곡', '을']\n",
      "21 ['▁쓰', '려는']\n",
      "23 ['▁뜻을']\n",
      "24 ['▁갖는다', '.']\n",
      "26 ['▁이']\n",
      "27 ['▁시기']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기(word) 단위로 쪼개진 context(word_tokens)를 Subword로 토큰화한 결과를 살펴봅니다. \n",
    "context_tokens, word_to_token = _tokenize_vocab(vocab, word_tokens)\n",
    "for i in range(min(20, len(word_to_token) - 1)):\n",
    "    print(word_to_token[i], context_tokens[word_to_token[i]:word_to_token[i + 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "caring-ridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14, '교향곡', ['교향곡을'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# answer_start와 answer_end로부터 word_start와 word_end를 구합니다. \n",
    "word_start = char_to_word[answer_start]\n",
    "word_end = char_to_word[answer_end]\n",
    "word_start, word_end, answer_text, word_tokens[word_start:word_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "expected-engineering",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 20, ['▁교향곡', '을'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = word_to_token[word_start]\n",
    "if word_end < len(word_to_token) - 1:\n",
    "    token_end = word_to_token[word_end + 1] - 1\n",
    "else:\n",
    "    token_end = len(context_tokens) - 1\n",
    "token_start, token_end, context_tokens[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "false-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁교향곡'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 실제 정답인 answer_text도 Subword 기준으로 토큰화해 둡니다. \n",
    "token_answer = \" \".join(vocab.encode_as_pieces(answer_text))\n",
    "token_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "resistant-deviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X >> (19, 20) ▁교향곡 을\n",
      "O >> (19, 19) ▁교향곡\n",
      "X >> (20, 20) 을\n"
     ]
    }
   ],
   "source": [
    "# 정답이 될수 있는 new_start와 new_end의 경우를 순회탐색합니다. \n",
    "for new_start in range(token_start, token_end + 1):\n",
    "    for new_end in range(token_end, new_start - 1, -1):\n",
    "        text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "        if text_span == token_answer:   # 정답과 일치하는 경우\n",
    "            print(\"O >>\", (new_start, new_end), text_span)\n",
    "        else:\n",
    "            print(\"X >>\", (new_start, new_end), text_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "mounted-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_tokens에서 char_answer의 위치를 찾아 리턴하는 함수\n",
    "def _improve_span(vocab, context_tokens, token_start, token_end, char_answer):\n",
    "    token_answer = \" \".join(vocab.encode_as_pieces(char_answer))\n",
    "    for new_start in range(token_start, token_end + 1):\n",
    "        for new_end in range(token_end, new_start - 1, -1):\n",
    "            text_span = \" \".join(context_tokens[new_start : (new_end + 1)])\n",
    "            if text_span == token_answer:\n",
    "                return (new_start, new_end)\n",
    "    return (token_start, token_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "affecting-paper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_start: 19  token_end: 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁교향곡']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, answer_text)\n",
    "print('token_start:', token_start, ' token_end:', token_end)\n",
    "context_tokens[token_start:token_end + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-reservation",
   "metadata": {},
   "source": [
    "# KorQuAD 데이터셋 전처리 (4) 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "level-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_korquad(vocab, json_data, out_file):\n",
    "    with open(out_file, \"w\") as f:\n",
    "        for data in tqdm(json_data[\"data\"]):\n",
    "            title = data[\"title\"]\n",
    "            for paragraph in data[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"]\n",
    "                context_words, char_to_word = _tokenize_whitespace(context)\n",
    "\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    assert len(qa[\"answers\"]) == 1\n",
    "                    qa_id = qa[\"id\"]\n",
    "                    question = qa[\"question\"]\n",
    "                    answer_text = qa[\"answers\"][0][\"text\"]\n",
    "                    answer_start = qa[\"answers\"][0][\"answer_start\"]\n",
    "                    answer_end = answer_start + len(answer_text) - 1\n",
    "\n",
    "                    assert answer_text == context[answer_start:answer_end + 1]\n",
    "\n",
    "                    word_start = char_to_word[answer_start]\n",
    "                    word_end = char_to_word[answer_end]\n",
    "\n",
    "                    word_answer = \" \".join(context_words[word_start:word_end + 1])\n",
    "                    char_answer = \" \".join(answer_text.strip().split())\n",
    "                    assert char_answer in word_answer\n",
    "\n",
    "                    context_tokens, word_to_token = _tokenize_vocab(vocab, context_words)\n",
    "\n",
    "                    token_start = word_to_token[word_start]\n",
    "                    if word_end < len(word_to_token) - 1:\n",
    "                        token_end = word_to_token[word_end + 1] - 1\n",
    "                    else:\n",
    "                        token_end = len(context_tokens) - 1\n",
    "\n",
    "                    token_start, token_end = _improve_span(vocab, context_tokens, token_start, token_end, char_answer)\n",
    "\n",
    "                    data = {\"qa_id\": qa_id, \"title\": title, \"question\": vocab.encode_as_pieces(question), \"context\": context_tokens, \"answer\": char_answer, \"token_start\": token_start, \"token_end\":token_end}\n",
    "                    f.write(json.dumps(data, ensure_ascii=False))\n",
    "                    f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "textile-soviet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bf0b5efe244702b3f9a8e1bc851b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3619d72dc224342b21bb54d2eb81636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 전처리를 수행하여 파일로 생성합니다. \n",
    "dump_korquad(vocab, train_json, f\"{data_dir}/korquad_train.json\")\n",
    "dump_korquad(vocab, dev_json, f\"{data_dir}/korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "signal-pencil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"qa_id\": \"6566495-0-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"를\", \"▁읽고\", \"▁무엇을\", \"▁쓰고\", \"자\", \"▁\", \"했\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"교향곡\", \"token_start\": 19, \"token_end\": 19}\n",
      "{\"qa_id\": \"6566495-0-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"는\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁어디\", \"까지\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"1악장\", \"token_start\": 168, \"token_end\": 169}\n",
      "{\"qa_id\": \"6566495-0-2\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"가\", \"▁\", \"파우스트\", \"▁서\", \"곡을\", \"▁쓸\", \"▁때\", \"▁어떤\", \"▁곡\", \"의\", \"▁영향을\", \"▁받았\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"베토벤의 교향곡 9번\", \"token_start\": 80, \"token_end\": 84}\n",
      "{\"qa_id\": \"6566518-0-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁1839\", \"년\", \"▁바그너\", \"가\", \"▁교향곡\", \"의\", \"▁소재로\", \"▁쓰\", \"려고\", \"▁했던\", \"▁책은\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"파우스트\", \"token_start\": 6, \"token_end\": 7}\n",
      "{\"qa_id\": \"6566518-0-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁\", \"파우스트\", \"▁서\", \"곡\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"이\", \"▁영향을\", \"▁받은\", \"▁베토벤\", \"의\", \"▁곡은\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"합창교향곡\", \"token_start\": 143, \"token_end\": 146}\n",
      "{\"qa_id\": \"5917067-0-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"가\", \"▁\", \"파우스트\", \"를\", \"▁처음으로\", \"▁읽\", \"은\", \"▁\", \"년\", \"도\", \"는\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"1839\", \"token_start\": 0, \"token_end\": 0}\n",
      "{\"qa_id\": \"5917067-0-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"가\", \"▁처음\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁한\", \"▁장소\", \"는\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"파리\", \"token_start\": 165, \"token_end\": 165}\n",
      "{\"qa_id\": \"5917067-0-2\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"의\", \"▁1\", \"악장\", \"의\", \"▁초연\", \"은\", \"▁어디서\", \"▁연주\", \"되었\", \"는\", \"가\", \"?\"], \"context\": [\"▁1839\", \"년\", \"▁바그너\", \"는\", \"▁괴테\", \"의\", \"▁\", \"파우스트\", \"을\", \"▁처음\", \"▁읽고\", \"▁그\", \"▁내용에\", \"▁마음이\", \"▁끌려\", \"▁이를\", \"▁소재로\", \"▁해서\", \"▁하나의\", \"▁교향곡\", \"을\", \"▁쓰\", \"려는\", \"▁뜻을\", \"▁갖는다\", \".\", \"▁이\", \"▁시기\", \"▁바그너\", \"는\", \"▁1838\", \"년에\", \"▁빛\", \"▁독\", \"촉\", \"으로\", \"▁산\", \"전\", \"수\", \"전을\", \"▁다\", \"▁\", \"걲\", \"은\", \"▁상황이\", \"라\", \"▁좌절\", \"과\", \"▁실망\", \"에\", \"▁가득\", \"했으며\", \"▁메\", \"피스\", \"토\", \"펠\", \"레스\", \"를\", \"▁만나는\", \"▁\", \"파우스트\", \"의\", \"▁심\", \"경에\", \"▁공감\", \"했다고\", \"▁한다\", \".\", \"▁또한\", \"▁파리에서\", \"▁아브\", \"네\", \"크의\", \"▁지휘\", \"로\", \"▁파리\", \"▁음악원\", \"▁관현악단\", \"이\", \"▁연주하는\", \"▁베토벤\", \"의\", \"▁교향곡\", \"▁9\", \"번을\", \"▁듣고\", \"▁깊은\", \"▁감\", \"명을\", \"▁받았는데\", \",\", \"▁이것이\", \"▁이듬해\", \"▁1\", \"월에\", \"▁\", \"파우스트\", \"의\", \"▁서\", \"곡으로\", \"▁쓰여진\", \"▁이\", \"▁작품에\", \"▁조금\", \"이라도\", \"▁영향을\", \"▁끼\", \"쳤\", \"으리라\", \"는\", \"▁것은\", \"▁의심\", \"할\", \"▁여지가\", \"▁없다\", \".\", \"▁여기\", \"의\", \"▁라\", \"단\", \"조\", \"▁조성\", \"의\", \"▁경우에도\", \"▁그의\", \"▁전기\", \"에\", \"▁적혀\", \"▁있는\", \"▁것처럼\", \"▁단순한\", \"▁정신적\", \"▁피로\", \"나\", \"▁실\", \"의\", \"가\", \"▁반영\", \"된\", \"▁것이\", \"▁아니라\", \"▁베토벤\", \"의\", \"▁합창\", \"교\", \"향\", \"곡\", \"▁조성\", \"의\", \"▁영향을\", \"▁받은\", \"▁것을\", \"▁볼\", \"▁수\", \"▁있다\", \".\", \"▁그렇게\", \"▁교향곡\", \"▁작곡\", \"을\", \"▁1839\", \"년부터\", \"▁40\", \"년에\", \"▁걸쳐\", \"▁파리에서\", \"▁착수\", \"했으나\", \"▁1\", \"악장\", \"을\", \"▁쓴\", \"▁뒤에\", \"▁중단\", \"했다\", \".\", \"▁또한\", \"▁작품의\", \"▁완성\", \"과\", \"▁동시에\", \"▁그는\", \"▁이\", \"▁서\", \"곡\", \"(1\", \"악장\", \")\", \"을\", \"▁파리\", \"▁음악원\", \"의\", \"▁연주회\", \"에서\", \"▁연주\", \"할\", \"▁파트\", \"보\", \"까지\", \"▁준비\", \"하였으나\", \",\", \"▁실제로는\", \"▁이루어지지\", \"는\", \"▁않았다\", \".\", \"▁결국\", \"▁초연\", \"은\", \"▁4\", \"년\", \"▁반\", \"이\", \"▁지난\", \"▁후에\", \"▁드레스덴\", \"에서\", \"▁연주\", \"되었고\", \"▁재\", \"연\", \"도\", \"▁이루어졌\", \"지만\", \",\", \"▁이후에\", \"▁그대로\", \"▁방치\", \"되고\", \"▁말았다\", \".\", \"▁그\", \"▁사이에\", \"▁그는\", \"▁리\", \"엔\", \"치\", \"와\", \"▁방\", \"황\", \"하는\", \"▁네덜란드\", \"인\", \"을\", \"▁완성\", \"하고\", \"▁탄\", \"호\", \"이\", \"저\", \"에도\", \"▁착수\", \"하는\", \"▁등\", \"▁분\", \"주\", \"한\", \"▁시간을\", \"▁보\", \"냈는데\", \",\", \"▁그런\", \"▁바쁜\", \"▁생활\", \"이\", \"▁이\", \"▁곡을\", \"▁잊\", \"게\", \"▁한\", \"▁것이\", \"▁아닌\", \"가\", \"▁하는\", \"▁의견도\", \"▁있다\", \".\"], \"answer\": \"드레스덴\", \"token_start\": 216, \"token_end\": 216}\n",
      "{\"qa_id\": \"6566495-1-0\", \"title\": \"파우스트_서곡\", \"question\": [\"▁바그너\", \"의\", \"▁작품을\", \"▁시인\", \"의\", \"▁피로\", \"▁쓰여\", \"졌다\", \"고\", \"▁극찬\", \"한\", \"▁것은\", \"▁누구\", \"인\", \"가\", \"?\"], \"context\": [\"▁한편\", \"▁1840\", \"년부터\", \"▁바그너\", \"와\", \"▁알고\", \"▁지내던\", \"▁리스트\", \"가\", \"▁잊\", \"혀\", \"져\", \"▁있던\", \"▁1\", \"악장\", \"을\", \"▁부활\", \"시켜\", \"▁1852\", \"년에\", \"▁바이마르\", \"에서\", \"▁연주\", \"했다\", \".\", \"▁이것을\", \"▁계기로\", \"▁바그너\", \"도\", \"▁이\", \"▁작품에\", \"▁다시\", \"▁관심을\", \"▁갖게\", \"▁되었고\", \",\", \"▁그\", \"▁해\", \"▁9\", \"월에는\", \"▁총\", \"보\", \"의\", \"▁반환\", \"을\", \"▁요구\", \"하여\", \"▁이를\", \"▁서\", \"곡으로\", \"▁간\", \"추\", \"린\", \"▁다음\", \"▁수정\", \"을\", \"▁했고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에서\", \"▁출판\", \"할\", \"▁개정\", \"판\", \"도\", \"▁준비\", \"했다\", \".\", \"▁1853\", \"년\", \"▁5\", \"월에는\", \"▁리스트\", \"가\", \"▁이\", \"▁작품이\", \"▁수정\", \"되었다\", \"는\", \"▁것을\", \"▁인정\", \"했지만\", \",\", \"▁끝내\", \"▁바그너\", \"의\", \"▁출판\", \"▁계획은\", \"▁무산\", \"되고\", \"▁말았다\", \".\", \"▁이후\", \"▁1855\", \"년에\", \"▁리스트\", \"가\", \"▁자신의\", \"▁작품\", \"▁\", \"파우스트\", \"▁교향곡\", \"을\", \"▁거의\", \"▁완성\", \"하여\", \"▁그\", \"▁사실을\", \"▁바그너\", \"에게\", \"▁알\", \"렸고\", \",\", \"▁바그너\", \"는\", \"▁다시\", \"▁개정된\", \"▁총\", \"보를\", \"▁리스트\", \"에게\", \"▁보내고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에는\", \"▁20\", \"루이\", \"의\", \"▁금\", \"을\", \"▁받고\", \"▁팔았다\", \".\", \"▁또한\", \"▁그의\", \"▁작품을\", \"▁“\", \"하나\", \"하나\", \"의\", \"▁음\", \"표\", \"가\", \"▁시인\", \"의\", \"▁피로\", \"▁쓰여\", \"졌다\", \"”\", \"며\", \"▁극찬\", \"했던\", \"▁한스\", \"▁폰\", \"▁\", \"뷜\", \"로\", \"가\", \"▁그것을\", \"▁피아노\", \"▁독주\", \"용으로\", \"▁편곡\", \"했는데\", \",\", \"▁리스트\", \"는\", \"▁그것을\", \"▁약간\", \"▁변형\", \"되었을\", \"▁뿐\", \"이라고\", \"▁지적했다\", \".\", \"▁이\", \"▁서\", \"곡\", \"의\", \"▁총\", \"보\", \"▁첫\", \"머리\", \"에는\", \"▁\", \"파우스트\", \"▁1\", \"부의\", \"▁내용\", \"▁중\", \"▁한\", \"▁구절\", \"을\", \"▁인용\", \"하고\", \"▁있다\", \".\"], \"answer\": \"한스 폰 뷜로\", \"token_start\": 164, \"token_end\": 168}\n",
      "{\"qa_id\": \"6566495-1-1\", \"title\": \"파우스트_서곡\", \"question\": [\"▁잊\", \"혀\", \"져\", \"▁있는\", \"▁\", \"파우스트\", \"▁서\", \"곡\", \"▁1\", \"악장\", \"을\", \"▁부활\", \"시킨\", \"▁것은\", \"▁누구\", \"인\", \"가\", \"?\"], \"context\": [\"▁한편\", \"▁1840\", \"년부터\", \"▁바그너\", \"와\", \"▁알고\", \"▁지내던\", \"▁리스트\", \"가\", \"▁잊\", \"혀\", \"져\", \"▁있던\", \"▁1\", \"악장\", \"을\", \"▁부활\", \"시켜\", \"▁1852\", \"년에\", \"▁바이마르\", \"에서\", \"▁연주\", \"했다\", \".\", \"▁이것을\", \"▁계기로\", \"▁바그너\", \"도\", \"▁이\", \"▁작품에\", \"▁다시\", \"▁관심을\", \"▁갖게\", \"▁되었고\", \",\", \"▁그\", \"▁해\", \"▁9\", \"월에는\", \"▁총\", \"보\", \"의\", \"▁반환\", \"을\", \"▁요구\", \"하여\", \"▁이를\", \"▁서\", \"곡으로\", \"▁간\", \"추\", \"린\", \"▁다음\", \"▁수정\", \"을\", \"▁했고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에서\", \"▁출판\", \"할\", \"▁개정\", \"판\", \"도\", \"▁준비\", \"했다\", \".\", \"▁1853\", \"년\", \"▁5\", \"월에는\", \"▁리스트\", \"가\", \"▁이\", \"▁작품이\", \"▁수정\", \"되었다\", \"는\", \"▁것을\", \"▁인정\", \"했지만\", \",\", \"▁끝내\", \"▁바그너\", \"의\", \"▁출판\", \"▁계획은\", \"▁무산\", \"되고\", \"▁말았다\", \".\", \"▁이후\", \"▁1855\", \"년에\", \"▁리스트\", \"가\", \"▁자신의\", \"▁작품\", \"▁\", \"파우스트\", \"▁교향곡\", \"을\", \"▁거의\", \"▁완성\", \"하여\", \"▁그\", \"▁사실을\", \"▁바그너\", \"에게\", \"▁알\", \"렸고\", \",\", \"▁바그너\", \"는\", \"▁다시\", \"▁개정된\", \"▁총\", \"보를\", \"▁리스트\", \"에게\", \"▁보내고\", \"▁브\", \"라이트\", \"코프\", \"흐\", \"▁&\", \"▁헤르\", \"텔\", \"▁출판사\", \"에는\", \"▁20\", \"루이\", \"의\", \"▁금\", \"을\", \"▁받고\", \"▁팔았다\", \".\", \"▁또한\", \"▁그의\", \"▁작품을\", \"▁“\", \"하나\", \"하나\", \"의\", \"▁음\", \"표\", \"가\", \"▁시인\", \"의\", \"▁피로\", \"▁쓰여\", \"졌다\", \"”\", \"며\", \"▁극찬\", \"했던\", \"▁한스\", \"▁폰\", \"▁\", \"뷜\", \"로\", \"가\", \"▁그것을\", \"▁피아노\", \"▁독주\", \"용으로\", \"▁편곡\", \"했는데\", \",\", \"▁리스트\", \"는\", \"▁그것을\", \"▁약간\", \"▁변형\", \"되었을\", \"▁뿐\", \"이라고\", \"▁지적했다\", \".\", \"▁이\", \"▁서\", \"곡\", \"의\", \"▁총\", \"보\", \"▁첫\", \"머리\", \"에는\", \"▁\", \"파우스트\", \"▁1\", \"부의\", \"▁내용\", \"▁중\", \"▁한\", \"▁구절\", \"을\", \"▁인용\", \"하고\", \"▁있다\", \".\"], \"answer\": \"리스트\", \"token_start\": 7, \"token_end\": 7}\n"
     ]
    }
   ],
   "source": [
    "def print_file(filename, count=10):\n",
    "    \"\"\"\n",
    "    파일 내용 출력\n",
    "    :param filename: 파일 이름\n",
    "    :param count: 출력 라인 수\n",
    "    \"\"\"\n",
    "    with open(filename) as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if count <= i:\n",
    "                break\n",
    "            print(line.strip())\n",
    "\n",
    "print_file(f\"{data_dir}/korquad_train.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-operations",
   "metadata": {},
   "source": [
    "# KorQuAD 데이터셋 전처리 (5) 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "floppy-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = os.path.join(data_dir, \"korquad_train.json\")\n",
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "proof-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_seq_length': 384, 'max_query_length': 64}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "\n",
    "args = Config({\n",
    "    'max_seq_length': 384,\n",
    "    'max_query_length': 64,\n",
    "})\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "arabic-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성한 데이터셋 파일을 메모리에 로딩하는 함수\n",
    "def load_data(args, filename):\n",
    "    inputs, segments, labels_start, labels_end = [], [], [], []\n",
    "\n",
    "    n_discard = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, desc=f\"Loading ...\")):\n",
    "            data = json.loads(line)\n",
    "            token_start = data.get(\"token_start\")\n",
    "            token_end = data.get(\"token_end\")\n",
    "            question = data[\"question\"][:args.max_query_length]\n",
    "            context = data[\"context\"]\n",
    "            answer_tokens = \" \".join(context[token_start:token_end + 1])\n",
    "            context_len = args.max_seq_length - len(question) - 3\n",
    "\n",
    "            if token_end >= context_len:\n",
    "                # 최대 길이내에 token이 들어가지 않은 경우 처리하지 않음\n",
    "                n_discard += 1\n",
    "                continue\n",
    "            context = context[:context_len]\n",
    "            assert len(question) + len(context) <= args.max_seq_length - 3\n",
    "\n",
    "            tokens = ['[CLS]'] + question + ['[SEP]'] + context + ['[SEP]']\n",
    "            ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "            ids += [0] * (args.max_seq_length - len(ids))\n",
    "            inputs.append(ids)\n",
    "            segs = [0] * (len(question) + 2) + [1] * (len(context) + 1)\n",
    "            segs += [0] * (args.max_seq_length - len(segs))\n",
    "            segments.append(segs)\n",
    "            token_start += (len(question) + 2)\n",
    "            labels_start.append(token_start)\n",
    "            token_end += (len(question) + 2)\n",
    "            labels_end.append(token_end)\n",
    "    print(f'n_discard: {n_discard}')\n",
    "\n",
    "    return (np.array(inputs), np.array(segments)), (np.array(labels_start), np.array(labels_end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "grand-spare",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cd79e7ec7d046468ad91fc551155117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discard: 430\n",
      "train_inputs: (59977, 384)\n",
      "train_inputs: (59977, 384)\n",
      "train_labels: (59977,)\n",
      "train_labels: (59977,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfec9202c2d4e228ecf32d6d54f7cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading ...: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_discard: 78\n",
      "dev_inputs: (5696, 384)\n",
      "dev_inputs: (5696, 384)\n",
      "dev_labels: (5696,)\n",
      "dev_labels: (5696,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([[    5, 15798,    10, ...,     0,     0,     0],\n",
       "         [    5, 15798,    10, ...,     0,     0,     0],\n",
       "         [    5, 15798,    19, ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    5, 21666,    19, ...,     0,     0,     0],\n",
       "         [    5,   964, 16865, ...,     0,     0,     0],\n",
       "         [    5,   365,    15, ...,     0,     0,     0]]),\n",
       "  array([[0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0],\n",
       "         [0, 0, 0, ..., 0, 0, 0]])),\n",
       " (array([ 37, 184,  98, ...,  74, 190,  35]),\n",
       "  array([ 37, 185, 102, ...,  75, 191,  44])))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train data load\n",
    "train_inputs, train_labels = load_data(args, train_json)\n",
    "print(f\"train_inputs: {train_inputs[0].shape}\")\n",
    "print(f\"train_inputs: {train_inputs[1].shape}\")\n",
    "print(f\"train_labels: {train_labels[0].shape}\")\n",
    "print(f\"train_labels: {train_labels[1].shape}\")\n",
    "\n",
    "# dev data load\n",
    "dev_inputs, dev_labels = load_data(args, dev_json)\n",
    "print(f\"dev_inputs: {dev_inputs[0].shape}\")\n",
    "print(f\"dev_inputs: {dev_inputs[1].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[0].shape}\")\n",
    "print(f\"dev_labels: {dev_labels[1].shape}\")\n",
    "\n",
    "train_inputs[:10], train_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "least-scale",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    5, 15798,    10, 28935,     9,    11, 29566,    20, 14604,\n",
       "       20424,  3904,    70,    11,  4648,    10,    19,  1910,     4,\n",
       "       22070,    15, 15798,    10, 28935,     9,    11, 29566,    16,\n",
       "         626, 14604,    38, 14028, 11773, 13829,   384,  8376,  3021,\n",
       "        1239,  6874,    16,  1687,  5958,  2694,  5061,     7,    30,\n",
       "        1613, 15798,    10, 28065,    75,  4415,  1816,  4978,    27,\n",
       "         347,   145,   107,  2703,   263,    11,     1,    18,  5853,\n",
       "          99,  9677,    24, 11969,    13,  7595,   437,  1019,  5907,\n",
       "         257,  3794,  1972,    20, 11278,    11, 29566,     9,   612,\n",
       "       12631, 13214,  1732,    76,     7,   110,  8802, 17581,   354,\n",
       "        9648,  2060,    21,  1682, 22110, 18164,    17, 21076, 14980,\n",
       "           9,  6874,    81, 11325,  4239,  3597,  1010,  1035, 17670,\n",
       "           8,  2447,  1306,    35,   443,    11, 29566,     9,   315,\n",
       "       12729, 14457,    30,  7938,  3742, 10766,   634,  9971, 17590,\n",
       "       19424,    10,   285,  4080,    61, 17573,   483,     7,  7588,\n",
       "           9,   473,   338,   147,  1924,     9, 11016,   136,  1034,\n",
       "          13, 11672,    40,  3436,  5217,  7898, 11684,    57,   830,\n",
       "           9,    19,  3319,    86,   220,   464, 14980,     9, 20515,\n",
       "         412,   991,   684,  1924,     9,   634,   920,   144,   430,\n",
       "          34,    25,     7,  4210,  6874,  2150,    16, 22070,   298,\n",
       "        1159,    75,  1098,  8802,  7490,   805,    35, 18678,    16,\n",
       "        1657,  1970,  2272,    53,     7,   110,  6559,  2178,    24,\n",
       "         756,    82,    30,   315,   684,  3772, 18678,    12,    16,\n",
       "        1682, 22110,     9, 22469,    22,  1757,    61,  8817,   194,\n",
       "         164,  1693,   749,     8,  6739, 12202,    10,   494,     7,\n",
       "         502, 12181,    18,    46,    15,   374,    17,  1680,   708,\n",
       "       26344,    22,  1757,   432,   465,   351,    32, 18563,   710,\n",
       "           8,  2585,  1384, 16071,   265,  3360,     7,    38,   747,\n",
       "          82,   383,   678,   200,    26,   590,  1281,    41,  1172,\n",
       "          31,    16,  2178,    43,  3044,   156,    17,   647,   468,\n",
       "        7490,    41,    84,   758,    92,    33,  3401,   369, 18319,\n",
       "           8,  2582, 29798,  1102,    17,    30,  4573, 11170,   139,\n",
       "          58,   220,   773,    19,   211, 23824,    25,     7,     4,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question과 Context가 포함된 입력데이터 1번째\n",
    "train_inputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "treated-newspaper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Question을 0으로, Context를 1로 구분해 준 Segment 데이터 1번째\n",
    "train_inputs[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "polar-indie",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 37)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer위치의 시작점과 끝점 라벨 1번째\n",
    "train_labels[0][0], train_labels[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-greensboro",
   "metadata": {},
   "source": [
    "# 2. BERT 모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "numeric-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유틸리티 함수들\n",
    "\n",
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "\n",
    "\n",
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "\n",
    "\n",
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "\n",
    "\n",
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "alone-disposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mode == \"embedding\" 일 경우 Token Embedding Layer 로 사용되는 layer 클래스입니다. \n",
    "\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shared Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hidden-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "alert-accordance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "mental-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "running-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "quarterly-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "buried-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, enc_tokens, segments):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_tokens: encoder tokens\n",
    "        :param segments: token segments\n",
    "        :return logits_cls: CLS 결과 logits\n",
    "        :return logits_lm: LM 결과 logits\n",
    "        \"\"\"\n",
    "        enc_self_mask = get_pad_mask(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = enc_out\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-conviction",
   "metadata": {},
   "source": [
    "# BERT 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "violent-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4KorQuAD(tf.keras.Model):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(name='BERT4KorQuAD')\n",
    "\n",
    "        self.bert = BERT(config)\n",
    "        self.dense = tf.keras.layers.Dense(2)\n",
    "    \n",
    "    def call(self, enc_tokens, segments):\n",
    "        logits_cls, logits_lm = self.bert(enc_tokens, segments)\n",
    "\n",
    "        hidden = self.dense(logits_lm) # (bs, n_seq, 2)\n",
    "        start_logits, end_logits = tf.split(hidden, 2, axis=-1)  # (bs, n_seq, 1), (bs, n_seq, 1)\n",
    "\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        start_outputs = tf.keras.layers.Softmax(name=\"start\")(start_logits)\n",
    "\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        end_outputs = tf.keras.layers.Softmax(name=\"end\")(end_logits)\n",
    "\n",
    "        return start_outputs, end_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "representative-happening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 512,\n",
       " 'n_head': 8,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 6,\n",
       " 'n_seq': 384,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = Config({\"d_model\": 512, \"n_head\": 8, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 6, \"n_seq\": 384, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "architectural-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_batch_size = 8  # 11GB 정도의 GPU 메모리에서 배치 사이즈는 32 정도가 적당할 것입니다. 하지만 8GB 정도의 GPU를 사용한다면 배치사이즈를 16 이하로 낮춰 주세\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_labels)).shuffle(10000).batch(bert_batch_size)\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_inputs, dev_labels)).batch(bert_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "buried-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERT4KorQuAD(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "acute-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, loss_fn, acc_fn, optimizer):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    p_bar = tqdm(dataset)\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(p_bar):\n",
    "        with tf.GradientTape() as tape:\n",
    "            start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "            start_loss = loss_fn(start_labels, start_outputs)\n",
    "            end_loss = loss_fn(end_labels, end_outputs)\n",
    "            loss = start_loss + end_loss\n",
    "\n",
    "            start_acc = acc_fn(start_labels, start_outputs)\n",
    "            end_acc = acc_fn(end_labels, end_outputs)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "        if batch % 10 == 9:\n",
    "            p_bar.set_description(f'loss: {metric_start_loss.result():0.4f}, {metric_end_loss.result():0.4f}, acc: {metric_start_acc.result():0.4f}, {metric_end_acc.result():0.4f}')\n",
    "    p_bar.close()\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "vietnamese-anniversary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, dataset, loss_fn, acc_fn):\n",
    "    metric_start_loss = tf.keras.metrics.Mean(name='start_loss')\n",
    "    metric_end_loss = tf.keras.metrics.Mean(name='end_loss')\n",
    "    metric_start_acc = tf.keras.metrics.Mean(name='start_acc')\n",
    "    metric_end_acc = tf.keras.metrics.Mean(name='end_acc')\n",
    "\n",
    "    for batch, ((enc_tokens, segments), (start_labels, end_labels)) in enumerate(dataset):\n",
    "        start_outputs, end_outputs = model(enc_tokens, segments)\n",
    "\n",
    "        start_loss = loss_fn(start_labels, start_outputs)\n",
    "        end_loss = loss_fn(end_labels, end_outputs)\n",
    "\n",
    "        start_acc = acc_fn(start_labels, start_outputs)\n",
    "        end_acc = acc_fn(end_labels, end_outputs)\n",
    "\n",
    "        metric_start_loss(start_loss)\n",
    "        metric_end_loss(end_loss)\n",
    "        metric_start_acc(start_acc)\n",
    "        metric_end_acc(end_acc)\n",
    "\n",
    "    return metric_start_loss.result(), metric_end_loss.result(), metric_start_acc.result(), metric_end_acc.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "disciplinary-police",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3514afffe55f45088f852a04436daaa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 0 >> loss: 5.9507, 5.9507, acc: 0.0039, 0.0035\n",
      "save best model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c679a6058142d5acfe00ba7d423b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 1 >> loss: 5.9507, 5.9507, acc: 0.0032, 0.0037\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808a89d9d51749c2aec1e963211e75a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval 2 >> loss: 5.9507, 5.9507, acc: 0.0004, 0.0019\n",
      "early stopping\n"
     ]
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "best_acc = .0\n",
    "patience = 0\n",
    "for epoch in range(20):\n",
    "    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)\n",
    "    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)\n",
    "    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')\n",
    "    acc = start_acc + end_acc\n",
    "    if best_acc < acc:\n",
    "        patience = 0\n",
    "        best_acc = acc\n",
    "        model.save_weights(os.path.join(data_dir, \"korquad_bert_none_pretrain.hdf5\"))\n",
    "        print(f'save best model')\n",
    "    else:\n",
    "        patience += 1\n",
    "    if 2 <= patience:\n",
    "        print(f'early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-shock",
   "metadata": {},
   "source": [
    "# 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-feeling",
   "metadata": {},
   "source": [
    "# 1. pretrained model 로딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "promotional-england",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"BERT4KorQuAD\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (BERT)                  multiple                  29202944  \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             multiple                  1026      \n",
      "=================================================================\n",
      "Total params: 29,203,970\n",
      "Trainable params: 29,203,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "checkpoint_file = os.path.join(model_dir, 'bert_pretrain_32000.hdf5')\n",
    "\n",
    "model = BERT4KorQuAD(config)\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    #  pretrained model 을 로드하기 위해 먼저 모델이 생성되어 있어야 한다.\n",
    "    enc_tokens = np.random.randint(0, len(vocab), (4, 10))\n",
    "    segments = np.random.randint(0, 2, (4, 10))\n",
    "    model(enc_tokens, segments)\n",
    "    \n",
    "    # checkpoint 파일로부터 필요한 layer를 불러온다. \n",
    "    model.load_weights(os.path.join(model_dir, \"bert_pretrain_32000.hdf5\"), by_name=True)\n",
    "\n",
    "    model.summary()\n",
    "else:\n",
    "    print('NO Pretrained Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smart-cream",
   "metadata": {},
   "source": [
    "# 2. petrained model finetune 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-rainbow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c8af67800c45a8b37a594e751be9ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7498 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.sparse_categorical_crossentropy\n",
    "acc_fn = tf.keras.metrics.sparse_categorical_accuracy\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "\n",
    "best_acc = .0\n",
    "patience = 0\n",
    "start_loss_list, end_loss_list, start_acc_list, end_acc_list = [], [], [], []\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_epoch(model, train_dataset, loss_fn, acc_fn, optimizer)\n",
    "    start_loss, end_loss, start_acc, end_acc = eval_epoch(model, dev_dataset, loss_fn, acc_fn)\n",
    "    print(f'eval {epoch} >> loss: {start_loss:0.4f}, {end_loss:0.4f}, acc: {start_acc:0.4f}, {end_acc:0.4f}')\n",
    "    start_loss_list.append(float(f'{start_loss:0.4f}'))\n",
    "    end_loss_list.append(float(f'{end_loss:0.4f}'))\n",
    "    start_acc_list.append(float(f'{start_acc:0.4f}'))\n",
    "    end_acc_list.append(float(f'{end_acc:0.4f}'))\n",
    "    acc = start_acc + end_acc\n",
    "    if best_acc < acc:\n",
    "        patience = 0\n",
    "        best_acc = acc\n",
    "        model.save_weights(os.path.join(data_dir, \"korquad_bert_none_pretrain.hdf5\"))\n",
    "        print(f'save best model')\n",
    "    else:\n",
    "        patience += 1\n",
    "    if 5 <= patience:\n",
    "        print(f'early stopping')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-physiology",
   "metadata": {},
   "source": [
    "# 3. Inference 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_predict(model, question, context):\n",
    "    \"\"\"\n",
    "    입력에 대한 답변 생성하는 함수\n",
    "    :param model: model\n",
    "    :param question: 입력 문자열\n",
    "    :param context: 입력 문자열\n",
    "    \"\"\"\n",
    "    q_tokens = vocab.encode_as_pieces(question)[:args.max_query_length]\n",
    "    c_tokens = vocab.encode_as_pieces(context)[:args.max_seq_length - len(q_tokens) - 3]\n",
    "    tokens = ['[CLS]'] + q_tokens + ['[SEP]'] + c_tokens + ['[SEP]']\n",
    "    token_ids = [vocab.piece_to_id(token) for token in tokens]\n",
    "    segments = [0] * (len(q_tokens) + 2) + [1] * (len(c_tokens) + 1)\n",
    "\n",
    "    y_start, y_end = model(np.array([token_ids]), np.array([segments]))\n",
    "    # print(y_start, y_end)\n",
    "    y_start_idx = K.argmax(y_start, axis=-1)[0].numpy()\n",
    "    y_end_idx = K.argmax(y_end, axis=-1)[0].numpy()\n",
    "    answer_tokens = tokens[y_start_idx:y_end_idx + 1]\n",
    "\n",
    "    return vocab.decode_pieces(answer_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_json = os.path.join(data_dir, \"korquad_dev.json\")\n",
    "\n",
    "with open(dev_json) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        data = json.loads(line)\n",
    "        question = vocab.decode_pieces(data['question'])\n",
    "        context = vocab.decode_pieces(data['context'])\n",
    "        answer = data['answer']\n",
    "        answer_predict = do_predict(model, question, context)\n",
    "        if answer in answer_predict:\n",
    "            print(i)\n",
    "            print(\"질문 : \", question)\n",
    "            print(\"지문 : \", context)\n",
    "            print(\"정답 : \", answer)\n",
    "            print(\"예측 : \", answer_predict, \"\\n\")\n",
    "        if 100 < i:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-tissue",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
