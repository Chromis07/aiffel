{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deeper_03 텍스트의 분포로 벡터화 하기   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word2Vec의 대중화 이전에, 텍스트의 분포를 활용하여 텍스트를 벡터화하는 아이디어를 들여다보자.   \n",
    "\n",
    "1. 단어 빈도를 이용한 벡터화   \n",
    "(1) Bag of Words   \n",
    "(2) Bag of Words 구현해보기   \n",
    "(3) DTM과 코사인 유사도   \n",
    "(4) DTM의 구현과 한계점   \n",
    "(5) TF-IDF   \n",
    "(6) TF-IDF 구현하기   \n",
    "2. LSA와 LDA   \n",
    "(1) LSA   \n",
    "(2) LSA 실습   \n",
    "(3) LDA   \n",
    "(4) LDA 실습   \n",
    "3. 텍스트 분포를 이용한 비지도 학습 토크나이저   \n",
    "(1) 형태소 분석기와 단어 미등록 문제   \n",
    "(2) soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텍스트를 벡터화하는 방법으로는 **(1) 통계와 머신러닝 활용**, **(2) 인공 신경망을 활용**하는 두가지 방법이 있다.   \n",
    "이번엔 전자의 방법으로 가보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW란, 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도(frequency)에만 집중하는 텍스트 데이터의 수치화 표현 방법이다. 텍스트를 전부 단어 단위로 토큰화 하고, 단어 사용 횟수를 카운트 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = 'John likes to watch movies. Mary likes movies too.'\n",
    "BoW1 = {\"John\":1, \"likes\":2, \"to\":1, \"watch\":1, \"movies\":2, \"Mary\":1, \"too\":1}\n",
    "\n",
    "doc2 = 'Mary also likes to watch football games.'\n",
    "BoW2 = {\"Mary\":1, \"also\":1, \"likes\":1, \"to\":1, \"watch\":1, \"football\":1, \"games\":1}\n",
    "\n",
    "# 순서는 다르지만 둘 다 같다\n",
    "BoW = {\"too\":1, \"Mary\":1, \"movies\":2, \"John\":1, \"watch\":1, \"likes\":2, \"to\":1}\n",
    "BoW1 = {\"John\":1, \"likes\":2, \"to\":1, \"watch\":1, \"movies\":2, \"Mary\":1, \"too\":1}\n",
    "\n",
    "doc3 = 'John likes to watch movies. Mary likes movies too. Mary also likes to watch football games.'\n",
    "BoW3 = {\"John\":1, \"likes\":3, \"to\":2, \"watch\":2, \"movies\":2, \"Mary\":2, \"too\":1, \"also\":1, \"football\":1, \"games\":1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어순이 달라지더라도 같은 문장으로 취급한다는 한계가 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Tokenizer로 Bag of Words 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words : {'john': 1, 'likes': 3, 'to': 2, 'watch': 2, 'movies': 2, 'mary': 2, 'too': 1, 'also': 1, 'football': 1, 'games': 1}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "# Keras Tokenizer로 Bag of Words 구현\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentence) # 텍스트를 리스트 형태로 단어장 생성 (중복 X)\n",
    "bow = dict(tokenizer.word_counts) # 각 단어와 각 단어의 빈도를 bow에 저장\n",
    "\n",
    "print(\"Bag of Words :\", bow) # bow 출력\n",
    "print('단어장(Vocabulary)의 크기 :', len(tokenizer.word_counts)) # 중복을 제거한 단어들의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn CountVectorizer로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[1 1 1 1 3 2 2 2 1 2]]\n",
      "각 단어의 인덱스 : {'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n",
      "단어장(Vocabulary)의 크기 : 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentence = [\"John likes to watch movies. Mary likes movies too! Mary also likes to watch football games.\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "bow = vector.fit_transform(sentence).toarray()\n",
    "\n",
    "print('Bag of Words : ', bow) # 코퍼스로부터 각 단어의 빈도수를 기록한다.\n",
    "print('각 단어의 인덱스 :', vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다.\n",
    "print('단어장(Vocabulary)의 크기 :', len(vector.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn의 CountVectorizer 빈도수만 나올 뿐이다.   \n",
    "Keras의 토크나이저를 사용하는 것이 보통이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTM(Document-Term Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTM이란, **여러 문서의 Bag of Words를 하나의 행렬로 구현**한 것이다.   \n",
    "즉, **각 문서에 등장한 단어의 빈도수를 하나의 행렬로 통합**한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc 1: Intelligent applications creates intelligent business processes   \n",
    "Doc 2: Bots are intelligent applications   \n",
    "Doc 3: I do business intelligence   \n",
    "\n",
    "![DTM](https://user-images.githubusercontent.com/59006548/145340168-a693bdfc-535d-48e6-b95d-97de94f53040.png)   \n",
    "위 문장들로 만들어진 DTM   \n",
    "   \n",
    "row는 문서 벡터(document vector), column은 단어 벡터(word vector)   \n",
    "문서 수가 많아지면 단어장이 커져서 희소벡터가 되어버린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도\n",
    "# 문서1 : I like dog\n",
    "# 문서2 : I like cat\n",
    "# 문서3 : I like cat I like cat\n",
    "# 위 문장의 DTM에서 코사인 유사도 계산\n",
    "# https://wikidocs.net/24603\n",
    "\n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "doc1 = np.array([0,1,1,1]) # 문서1 벡터\n",
    "doc2 = np.array([1,0,1,1]) # 문서2 벡터\n",
    "doc3 = np.array([2,0,2,2]) # 문서3 벡터\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![코사인유사도](https://wikidocs.net/images/page/24603/%EC%BD%94%EC%82%AC%EC%9D%B8%EC%9C%A0%EC%82%AC%EB%8F%84.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![코사인 유사도](https://i.imgur.com/C6VnrI4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666667\n",
      "0.6666666666666667\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "print(cos_sim(doc1, doc2)) #문서1과 문서2의 코사인 유사도\n",
    "print(cos_sim(doc1, doc3)) #문서1과 문서3의 코사인 유사도\n",
    "print(cos_sim(doc2, doc3)) #문서2과 문서3의 코사인 유사도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`문서1`과 `문서2`의 코사인 유사도는 0.67, `문서1`과 `문서3`의 코사인 유사도도 0.67      \n",
    "`문서2`와 `문서3`의 유사도는 1 (모든 단어의 빈도수가 동일하게 증가했기 때문)    \n",
    "코사인 유사도는 벡터의 크기가 아니라 벡터의 방향(패턴)에 초첨을 둔다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 1 1 0 1]\n",
      " [0 0 0 0 1 1 1 0 1 0]\n",
      " [1 1 1 0 1 1 0 1 0 1]]\n",
      "{'john': 3, 'likes': 4, 'to': 7, 'watch': 9, 'movies': 6, 'mary': 5, 'too': 8, 'also': 0, 'football': 1, 'games': 2}\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer로 DTM을 만들자\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'John likes to watch movies',\n",
    "    'Mary likes movies too',\n",
    "    'Mary also likes to watch football games',    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "\n",
    "print(vector.fit_transform(corpus).toarray()) # 코퍼스로부터 각 단어의 빈도수를 기록.\n",
    "print(vector.vocabulary_) # 각 단어의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM의 한계\n",
    "\n",
    "1. DTM의 문서 수와 단어 수가 늘어날 수록 벡터가 쓸데없이 커진다. (희소벡터, 차원의 저주)\n",
    "2. 단어의 빈도에만 집중하는 방법이기에 한계가 있다. `the`가 많이 있다고 해서 유사한 문장 X   \n",
    "- 그렇다면, 중요한 단어와 중요하지 않은 단어에 가중치를 따로 선별하는 방법은?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`TF-IDF`(Term Frequency-Inverse Document Frequency)는 모든 문서에서 자주 등장하는 단어는 중요도를 낮게 보고, 특정 문서에서만 자주 등장하는 단어는 중요도를 높게 본다. 마치 불용어를 제외하고 보듯이. -> IDF 항에서 이 역할을 수행   \n",
    "하지만 이것이 DTM보다 성능이 항상 좋지는 않다.   \n",
    "DTM을 만든 뒤 TF-IDF 가중치를 DTM에 적용   \n",
    "사실 DTM 자체가 이미 TF (Term Frequency)   \n",
    "![TF-IDF](https://img1.daumcdn.net/thumb/R800x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbKcggU%2FbtqCkQ2NEH1%2FAp9xO7HQSDzfKixMyuGNCk%2Fimg.png)   \n",
    "tf 뒤에 곱해지는 log항이 IDF.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 문서의 수가 5개라고 해봅시다. 그리고 단어 **'like'가 문서2에서 200번**, **문서 3에서 300번** 등장했다고 해봅시다. 다른 문서에서 단어 `'like'`는 등장하지 않았습니다. 이때, **단어 'like'의 IDF**는 몇일까요?\n",
    "($$tf1=200,tf2=300, N=500, df=200 $$)\n",
    "\n",
    "$$IDF = log(500/200) = ln(5/2) = 0.91629073187$$   \n",
    "그러면 여기서 문서2와 문서3의 단어 `'like'`의 TF-IDF의 값은?   \n",
    "$$문서2 TF-IDF = 200 * ln(5/2) = 183.258146375$$\n",
    "$$문서3 TF-IDF = 300 * ln(5/2) = 274.887219562$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에꺼 계산 이거 맞아??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bloter.net/newsView/blt201609280001\n",
    "희소벡터를 해결하기 위해 특이값분해를 통해 축소시킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
