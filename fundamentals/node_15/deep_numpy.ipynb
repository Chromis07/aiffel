{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8579 - accuracy: 0.8011\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2493 - accuracy: 0.9297\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1885 - accuracy: 0.9457\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1588 - accuracy: 0.9555\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1337 - accuracy: 0.9616\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1157 - accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1005 - accuracy: 0.9709\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0906 - accuracy: 0.9747\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0815 - accuracy: 0.9774\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0756 - accuracy: 0.9793\n",
      "313/313 - 1s - loss: 0.1060 - accuracy: 0.9684\n",
      "test_loss: 0.10600697249174118 \n",
      "test_accuracy: 0.9684000015258789\n"
     ]
    }
   ],
   "source": [
    "# 평범한 tensorflow 활용한 mnist 코드\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개 이상의 레이어를 쌓아 만든 것을 다층 퍼셉트론 (Multio-Layer Perceptron; MLP) 더 깊어질 수록 deep 하다   \n",
    "보통 이것을 DNN 이라고 부른다   \n",
    "Fully-Connected Neural Network - 서로 다른 층에 위치한 노드와는 연결 관계가 없고, 인접한 층의 노드들만 연결   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-20 15:30:06.497623: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X=x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "WEIGT_INIT_STD = 0.1\n",
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "# np.random.randn(m, n) : 평균 0, 표준편차 1의 가우시안 표준정규분포 난수를\n",
    "# matrix array(m, n) 생성\n",
    "W1 = WEIGT_INIT_STD * np.random.randn(INPUT_SIZE, HIDDEN_SIZE)  \n",
    "# bias 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(HIDDEN_SIZE)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.63315340e-01,  4.12567543e-01, -3.04813543e-01,  1.36268976e+00,\n",
       "        3.22801035e-01, -5.44425574e-01, -4.90577199e-01, -3.26182010e-01,\n",
       "        1.66256334e-01,  2.62710612e-02,  9.19342698e-01,  9.59103027e-01,\n",
       "       -2.50801740e-01,  1.22884698e+00, -6.32247972e-01, -6.08466103e-01,\n",
       "        6.64044197e-01,  1.73433455e-01,  9.51240857e-01,  8.36139134e-02,\n",
       "        7.17880412e-01, -2.01048774e+00, -4.88704375e-01,  7.54879846e-04,\n",
       "        7.77322713e-01,  4.66057293e-01, -3.38112922e+00,  1.68680236e+00,\n",
       "        7.66138233e-01,  7.77960435e-01, -8.74642537e-01,  1.38583591e+00,\n",
       "        1.93723113e-01, -3.91851803e-01,  1.93209934e-01,  2.12234064e-01,\n",
       "       -1.14647865e+00,  3.33189651e-01, -7.28379379e-01,  1.32925949e+00,\n",
       "       -1.56580560e-02,  6.39346601e-01,  1.14255703e+00, -4.74488630e-01,\n",
       "       -1.37163974e-01,  3.63165311e-01,  1.27462102e+00, -9.52524069e-01,\n",
       "       -3.28961356e-02, -1.75095378e-01])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해보자. 50 dim\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수 (Activation Functions)\n",
    "nonlinear한 출력으로 만들어줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61380037 0.60170337 0.4243812  0.79619651 0.58000673 0.36715868\n",
      " 0.3797576  0.41916989 0.54146861 0.50656739 0.71490816 0.72294218\n",
      " 0.43762617 0.7736167  0.34700099 0.35240918 0.66016828 0.54325001\n",
      " 0.72136466 0.52089131 0.6721401  0.11810617 0.38019883 0.50018872\n",
      " 0.68510281 0.61445015 0.03289046 0.84380318 0.68268492 0.68524037\n",
      " 0.29428921 0.79992664 0.54827988 0.4032716  0.54815278 0.55286025\n",
      " 0.24113286 0.58253526 0.32555046 0.79071812 0.49608557 0.65460574\n",
      " 0.7581488  0.3835544  0.46576267 0.58980645 0.78153276 0.27837749\n",
      " 0.49177671 0.45633765]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://reniew.github.io/12/\n",
    "https://pozalabs.github.io/Activation_Function/\n",
    "\n",
    "1. 하이퍼볼릭 탄젠트 (tanh)\n",
    "2. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21283341 -0.32247143  0.25920851 -0.17063848 -0.6178833   0.67250877\n",
      " -0.10709499  0.19553683  0.64657685  0.37855783]\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 50\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "W1 = WEIGT_INIT_STD * np.random.randn(INPUT_SIZE, HIDDEN_SIZE)\n",
    "b1 = np.zeros(HIDDEN_SIZE)\n",
    "W2 = WEIGT_INIT_STD * np.random.randn(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "b2 = np.zeros(OUTPUT_SIZE)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 OUTPUT_SIZE만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06936044, 0.06215794, 0.11120314, 0.07234972, 0.0462595 ,\n",
       "       0.16811678, 0.07709628, 0.10434335, 0.16381323, 0.12529963])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수\n",
    "https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718   \n",
    "Loss / Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 - 원 핫 인코딩\n",
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_one_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06936044 0.06215794 0.11120314 0.07234972 0.0462595  0.16811678\n",
      " 0.07709628 0.10434335 0.16381323 0.12529963]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])\n",
    "# 아직은 정답과는 멀다. 대부분 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.510560713162438\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "print(Loss)\n",
    "# 이제 이 오차를 줄여보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법\n",
    "Gradient Descent   \n",
    "\n",
    "https://aileen93.tistory.com/71   \n",
    "https://reniew.github.io/13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01387209,  0.01243159,  0.02224063,  0.01446994,  0.0092519 ,\n",
       "        -0.16637664,  0.01541926,  0.02086867,  0.03276265,  0.02505993],\n",
       "       [-0.1839906 ,  0.01433841,  0.02333718,  0.01413361,  0.00992812,\n",
       "         0.03940503,  0.01583154,  0.01850919,  0.03088004,  0.01762748],\n",
       "       [ 0.01509773,  0.01301962,  0.02357189,  0.01569102, -0.19018926,\n",
       "         0.03591147,  0.01345941,  0.01888795,  0.03198581,  0.02256437],\n",
       "       [ 0.01423034, -0.18919411,  0.02218519,  0.01512545,  0.01024672,\n",
       "         0.03258061,  0.0157972 ,  0.0199486 ,  0.03718204,  0.02189795],\n",
       "       [ 0.01488176,  0.0142834 ,  0.0220065 ,  0.01622971,  0.0109531 ,\n",
       "         0.03471935,  0.01856496,  0.01808552,  0.0304532 , -0.18017751]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01387209,  0.01243159,  0.02224063,  0.01446994,  0.0092519 ,\n",
       "        -0.16637664,  0.01541926,  0.02086867,  0.03276265,  0.02505993],\n",
       "       [-0.1839906 ,  0.01433841,  0.02333718,  0.01413361,  0.00992812,\n",
       "         0.03940503,  0.01583154,  0.01850919,  0.03088004,  0.01762748],\n",
       "       [ 0.01509773,  0.01301962,  0.02357189,  0.01569102, -0.19018926,\n",
       "         0.03591147,  0.01345941,  0.01888795,  0.03198581,  0.02256437],\n",
       "       [ 0.01423034, -0.18919411,  0.02218519,  0.01512545,  0.01024672,\n",
       "         0.03258061,  0.0157972 ,  0.0199486 ,  0.03718204,  0.02189795],\n",
       "       [ 0.01488176,  0.0142834 ,  0.0220065 ,  0.01622971,  0.0109531 ,\n",
       "         0.03471935,  0.01856496,  0.01808552,  0.0304532 , -0.18017751]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat -t) / batch_num\n",
    "dy # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01022122, -0.0973856 ,  0.0422367 ,  0.02889826, -0.04104455,\n",
       "         0.02272346,  0.03048704,  0.03602699,  0.06234168, -0.07406277],\n",
       "       [-0.09766386, -0.10080256,  0.07604881,  0.05064675, -0.08220126,\n",
       "        -0.01585542,  0.053239  ,  0.06470221,  0.10983811, -0.0579518 ],\n",
       "       [-0.03780634, -0.10747497,  0.07298723,  0.0488542 , -0.1428833 ,\n",
       "        -0.0183457 ,  0.04995405,  0.06224179,  0.10595596, -0.03348291],\n",
       "       [-0.05166253, -0.07385317,  0.04441939,  0.02991886, -0.06171192,\n",
       "         0.03915615,  0.03133707,  0.03727777,  0.06415531, -0.05903693],\n",
       "       [-0.07047946, -0.08387616,  0.06262421,  0.04183992, -0.10277732,\n",
       "         0.01605836,  0.04338249,  0.05285661,  0.09014291, -0.04977156],\n",
       "       [-0.04716135, -0.10171745,  0.06399824,  0.04258134, -0.11930311,\n",
       "        -0.02088364,  0.04353996,  0.05467023,  0.0931125 , -0.00883671],\n",
       "       [-0.11629521, -0.09634314,  0.0721907 ,  0.04801194, -0.05038333,\n",
       "        -0.00695382,  0.05107695,  0.06131665,  0.10413989, -0.06676063],\n",
       "       [-0.11738977, -0.11001203,  0.06335671,  0.04166709, -0.0232778 ,\n",
       "        -0.03334795,  0.04439924,  0.0543784 ,  0.09243251, -0.01220638],\n",
       "       [-0.02726916, -0.11225817,  0.06612138,  0.04429038, -0.08422737,\n",
       "        -0.05493579,  0.04598938,  0.05713953,  0.09703886, -0.03188904],\n",
       "       [-0.0876609 , -0.02242235,  0.04973342,  0.03292867, -0.07057023,\n",
       "        -0.00428222,  0.03453516,  0.04175493,  0.0701632 , -0.04417967],\n",
       "       [-0.0581579 , -0.09015036,  0.04794885,  0.03193211, -0.00362526,\n",
       "        -0.04120607,  0.03427838,  0.04154986,  0.07059189, -0.03316149],\n",
       "       [-0.01782767, -0.0782324 ,  0.03904943,  0.02613298, -0.10005137,\n",
       "         0.01934586,  0.02620823,  0.03304597,  0.0570001 , -0.00467113],\n",
       "       [-0.01071687, -0.0946091 ,  0.04999357,  0.034185  , -0.05295897,\n",
       "         0.01332058,  0.03604493,  0.0426392 ,  0.07325942, -0.09115775],\n",
       "       [-0.05026818, -0.10682061,  0.05875814,  0.03926704, -0.0956424 ,\n",
       "         0.00694527,  0.04051079,  0.04998381,  0.08566397, -0.02839783],\n",
       "       [-0.03811801, -0.06979232,  0.05146755,  0.03455075, -0.09461734,\n",
       "         0.0075708 ,  0.03561188,  0.04355673,  0.07426902, -0.04449907],\n",
       "       [-0.05571178, -0.09146471,  0.06653605,  0.04425391, -0.13302083,\n",
       "        -0.01350272,  0.04518649,  0.05656999,  0.09619332, -0.01503972],\n",
       "       [-0.06219976, -0.04501134,  0.06438356,  0.042874  , -0.09600049,\n",
       "        -0.04974773,  0.04461326,  0.05490632,  0.09210944, -0.04592727],\n",
       "       [-0.11645042, -0.06809434,  0.06635933,  0.0436723 , -0.06741433,\n",
       "        -0.03426698,  0.04594128,  0.05646657,  0.09520381, -0.02141722],\n",
       "       [-0.11012717, -0.04446479,  0.06202415,  0.04072304, -0.08434534,\n",
       "        -0.03353346,  0.04248696,  0.05257908,  0.08830516, -0.01364763],\n",
       "       [-0.09837367, -0.02276381,  0.05809457,  0.03856981, -0.08586957,\n",
       "         0.00045304,  0.04044569,  0.04868046,  0.08181395, -0.06105047],\n",
       "       [-0.10287476, -0.1013867 ,  0.0833451 ,  0.05524434, -0.12191331,\n",
       "        -0.02999968,  0.05734864,  0.07092815,  0.120126  , -0.03081776],\n",
       "       [-0.12161502, -0.08174297,  0.07297211,  0.04879999, -0.07365934,\n",
       "         0.03838153,  0.05178371,  0.06119728,  0.10422495, -0.10034222],\n",
       "       [-0.11028563, -0.08453117,  0.06019591,  0.03962689, -0.09253203,\n",
       "         0.01943722,  0.04109095,  0.05066811,  0.08645515, -0.01012539],\n",
       "       [-0.07238995, -0.07792846,  0.0790696 ,  0.05273926, -0.10474275,\n",
       "        -0.05586274,  0.05504868,  0.06759794,  0.11386354, -0.05739511],\n",
       "       [-0.03465874, -0.00457687,  0.04339122,  0.02919397, -0.0976665 ,\n",
       "        -0.0008663 ,  0.03005221,  0.03631047,  0.06087988, -0.06205933],\n",
       "       [-0.01503312, -0.03373678,  0.03507827,  0.0236256 , -0.08625092,\n",
       "         0.0057362 ,  0.02399864,  0.02954373,  0.05017448, -0.03313609],\n",
       "       [-0.11971082, -0.07594253,  0.06823958,  0.04466107, -0.05186355,\n",
       "        -0.07340298,  0.04709801,  0.05864966,  0.09851806,  0.0037535 ],\n",
       "       [-0.06051843, -0.10852184,  0.0770303 ,  0.05164798, -0.11543578,\n",
       "        -0.01006306,  0.05365681,  0.06555422,  0.11155937, -0.06490957],\n",
       "       [-0.09651725, -0.09486107,  0.06972823,  0.04666608, -0.0675931 ,\n",
       "         0.01607387,  0.04938294,  0.05895009,  0.10047064, -0.08230044],\n",
       "       [-0.11443472, -0.05790234,  0.0728516 ,  0.04826239, -0.10504457,\n",
       "        -0.00924277,  0.0504069 ,  0.06145085,  0.10369512, -0.05004245],\n",
       "       [-0.0567899 , -0.04763722,  0.03928191,  0.02583039, -0.03728212,\n",
       "        -0.04031331,  0.02708704,  0.03378895,  0.05687683, -0.00084257],\n",
       "       [-0.10101338, -0.05702937,  0.05498098,  0.03615384, -0.06198454,\n",
       "        -0.01840466,  0.03793588,  0.04663137,  0.07874945, -0.01601958],\n",
       "       [-0.02856981, -0.0453045 ,  0.02366347,  0.01585404, -0.01291629,\n",
       "        -0.00016463,  0.01688167,  0.02022561,  0.03463179, -0.02430135],\n",
       "       [-0.10789658,  0.00086196,  0.06532047,  0.04334204, -0.09007971,\n",
       "        -0.02887484,  0.04562928,  0.05491773,  0.09147418, -0.07469453],\n",
       "       [-0.10766825, -0.05072363,  0.05885818,  0.03877873, -0.094001  ,\n",
       "         0.00527943,  0.0402949 ,  0.04944458,  0.0836404 , -0.02390334],\n",
       "       [-0.09257132, -0.06245018,  0.0577675 ,  0.03853781, -0.03682656,\n",
       "        -0.00596755,  0.0411892 ,  0.04896013,  0.08288797, -0.07152701],\n",
       "       [-0.00852469, -0.08602231,  0.04067128,  0.02798198, -0.06549344,\n",
       "         0.05408689,  0.02919009,  0.03415231,  0.05939294, -0.08543504],\n",
       "       [-0.10892165, -0.03668493,  0.06721152,  0.04445078, -0.09253322,\n",
       "        -0.025353  ,  0.04654132,  0.0567707 ,  0.09528922, -0.04677074],\n",
       "       [-0.0517862 , -0.05561898,  0.07864858,  0.05276265, -0.13676543,\n",
       "        -0.04161888,  0.05464706,  0.06685884,  0.11245549, -0.07958313],\n",
       "       [-0.00102505, -0.05701835,  0.02672895,  0.01809946, -0.03635218,\n",
       "        -0.00850659,  0.01876777,  0.02304519,  0.03952746, -0.02326666],\n",
       "       [-0.04943214, -0.04317677,  0.0497959 ,  0.03325207, -0.07422441,\n",
       "        -0.02123408,  0.03464065,  0.04231018,  0.07133046, -0.04326186],\n",
       "       [-0.0114625 , -0.0524567 ,  0.03953904,  0.02687145, -0.07417618,\n",
       "         0.01209144,  0.02777025,  0.03342483,  0.05707918, -0.05868081],\n",
       "       [-0.04720262, -0.08562379,  0.05830394,  0.03930975, -0.06747739,\n",
       "         0.00268723,  0.04132915,  0.04956101,  0.08451082, -0.0753981 ],\n",
       "       [-0.08311997, -0.05279955,  0.05252661,  0.03491172, -0.08443537,\n",
       "         0.02105384,  0.03634923,  0.04401878,  0.07481464, -0.04331991],\n",
       "       [-0.0822682 , -0.03988735,  0.04056147,  0.02664871, -0.03914561,\n",
       "        -0.0104304 ,  0.02812633,  0.03433524,  0.05796524, -0.01590543],\n",
       "       [-0.07415932, -0.0595042 ,  0.04867839,  0.03229917, -0.06484116,\n",
       "         0.00382499,  0.03376406,  0.04112618,  0.06988771, -0.03107581],\n",
       "       [-0.07139723, -0.06866554,  0.07312141,  0.04884841, -0.13064553,\n",
       "        -0.00873694,  0.05049522,  0.06183361,  0.10462672, -0.05948012],\n",
       "       [-0.01950167, -0.10378267,  0.0392749 ,  0.02639888, -0.0203865 ,\n",
       "        -0.02014103,  0.02786985,  0.03415296,  0.05877902, -0.02266374],\n",
       "       [-0.08697167, -0.02291206,  0.04473854,  0.02932913, -0.02095256,\n",
       "        -0.0571734 ,  0.03135547,  0.03835047,  0.06380888, -0.0195728 ],\n",
       "       [-0.05702791, -0.05306763,  0.04569705,  0.030285  , -0.04989152,\n",
       "        -0.03233606,  0.03174533,  0.03911311,  0.06599158, -0.02050896]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW2 = np.dot(z1.T, dy)    \n",
    "dW2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW2 = np.dot(z1.T, dy)\n",
    "db2 = np.sum(dy, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(x):\n",
    "    return (1.0 - sigmoid(x)) * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz1 = np.dot(dy, W2.T)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dW1 = np.dot(X.T, da1)\n",
    "db1 = np.sum(dz1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10948105 0.1098637  0.10721648 0.0765648  0.1284041  0.19490329\n",
      "  0.04847601 0.10367837 0.05369728 0.06771493]\n",
      " [0.09775628 0.11266899 0.10090375 0.06121997 0.13763614 0.21295546\n",
      "  0.04875051 0.09919768 0.06398848 0.06492273]\n",
      " [0.12563775 0.11649893 0.10457832 0.07721386 0.11725622 0.17420178\n",
      "  0.0577359  0.10297777 0.0545546  0.06934486]\n",
      " [0.11961497 0.10103441 0.08865831 0.07807109 0.12503143 0.21391614\n",
      "  0.05888162 0.09579128 0.04977706 0.06922369]\n",
      " [0.11009738 0.096251   0.12540097 0.08209934 0.14224166 0.17196689\n",
      "  0.04966646 0.0994686  0.05284783 0.06995987]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.211210200285573\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "W1 = WEIGT_INIT_STD * np.random.randn(INPUT_SIZE, HIDDEN_SIZE)\n",
    "b1 = np.zeros(HIDDEN_SIZE)\n",
    "W2 = WEIGT_INIT_STD * np.random.randn(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "b2 = np.zeros(OUTPUT_SIZE)\n",
    "\n",
    "# Forward Propagation\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "\n",
    "# 추론과 오차(Loss) 계산\n",
    "y_hat = softmax(a2)\n",
    "t = _change_one_hot_label(Y_digit, 10)   # 정답 One-hot 인코딩\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "print(y_hat)\n",
    "print(t)\n",
    "print('Loss: ', Loss)\n",
    "        \n",
    "dy = (y_hat - t) / X.shape[0]\n",
    "dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "da1 = sigmoid_grad(a1) * dz1\n",
    "dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "\n",
    "# 경사하강법을 통한 파라미터 업데이트    \n",
    "learning_rate = 0.1\n",
    "W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = WEIGT_INIT_STD * np.random.randn(INPUT_SIZE, HIDDEN_SIZE)\n",
    "b1 = np.zeros(HIDDEN_SIZE)\n",
    "W2 = WEIGT_INIT_STD * np.random.randn(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "b2 = np.zeros(OUTPUT_SIZE)\n",
    "\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    z1 = sigmoid(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_one_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    da1 = sigmoid_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------\n",
      "[[0.05329784 0.18300357 0.07334883 0.07266068 0.11142243 0.04807258\n",
      "  0.11299143 0.12410736 0.11765101 0.10344427]\n",
      " [0.04704412 0.19584562 0.05930816 0.06631135 0.1179936  0.03477827\n",
      "  0.12305053 0.14118924 0.12020245 0.09427666]\n",
      " [0.0545041  0.17387154 0.0803206  0.06931697 0.12576656 0.05502122\n",
      "  0.11463467 0.12795124 0.11236331 0.0862498 ]\n",
      " [0.05728375 0.20350763 0.07331705 0.06420148 0.10141083 0.03603776\n",
      "  0.13149275 0.12989703 0.11254147 0.09031025]\n",
      " [0.05723744 0.17528686 0.05191575 0.07001755 0.11793383 0.03656092\n",
      "  0.12156407 0.15103921 0.12184859 0.09659577]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.418862535455883\n",
      "---------\n",
      "[[0.0688426  0.18406041 0.0675754  0.06675389 0.12570096 0.06376175\n",
      "  0.09662957 0.10362442 0.10096901 0.12208201]\n",
      " [0.06539252 0.19913323 0.05440873 0.06032941 0.13588917 0.04616033\n",
      "  0.10441974 0.11637903 0.10225998 0.11562786]\n",
      " [0.06896833 0.17602558 0.07328586 0.06376318 0.14881284 0.06863101\n",
      "  0.09686568 0.10580296 0.0954394  0.10240516]\n",
      " [0.07310559 0.21744828 0.06735644 0.05879714 0.11494719 0.04521473\n",
      "  0.11124058 0.1081553  0.0957298  0.10800495]\n",
      " [0.0747458  0.17996164 0.04781674 0.06418386 0.13576689 0.04668409\n",
      "  0.1024417  0.1244174  0.10350419 0.1204777 ]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.2054199831998957\n",
      "---------\n",
      "[[0.0846814  0.18066029 0.06156921 0.06067339 0.13563934 0.08093647\n",
      "  0.08341066 0.08798875 0.08740683 0.13703367]\n",
      " [0.08592487 0.19632141 0.04914806 0.05407977 0.14841141 0.05824202\n",
      "  0.08928485 0.09739122 0.08752652 0.13366987]\n",
      " [0.08333016 0.17361315 0.06607181 0.05794195 0.16828954 0.08219677\n",
      "  0.08246999 0.0888078  0.08155774 0.11572109]\n",
      " [0.08907875 0.22637169 0.06115815 0.0532219  0.12446501 0.05445404\n",
      "  0.0949142  0.09147319 0.08198996 0.12287313]\n",
      " [0.09258768 0.17932551 0.04337116 0.05794905 0.14847266 0.0568649\n",
      "  0.08690927 0.1040076  0.08839354 0.14211863]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  2.0374224308230833\n",
      "---------\n",
      "[[0.10002639 0.17535387 0.05582456 0.05489889 0.14193794 0.09918689\n",
      "  0.07263384 0.07570504 0.07630742 0.14812516]\n",
      " [0.10771694 0.19067495 0.04405631 0.04813693 0.15620632 0.07054761\n",
      "  0.07694216 0.08250437 0.0754357  0.14777872]\n",
      " [0.09687603 0.16918396 0.0592863  0.05239034 0.1844471  0.09528087\n",
      "  0.07081142 0.07551286 0.07022697 0.12598415]\n",
      " [0.10439696 0.23287611 0.05526907 0.04795198 0.1305474  0.0634443\n",
      "  0.08172705 0.0783994  0.07079138 0.13459635]\n",
      " [0.10985531 0.17623972 0.03907158 0.0519788  0.15675904 0.06675021\n",
      "  0.0743392  0.08808848 0.07604888 0.16086878]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.902761089971404\n",
      "---------\n",
      "[[0.11418742 0.1695282  0.05055713 0.04963203 0.14551415 0.11808653\n",
      "  0.06376499 0.06585099 0.06714647 0.1557321 ]\n",
      " [0.12977225 0.18407583 0.03937907 0.04274968 0.16037213 0.08262401\n",
      "  0.06682074 0.07063346 0.06548219 0.15809065]\n",
      " [0.10901638 0.16418948 0.05318057 0.04734425 0.19801424 0.10751143\n",
      "  0.06133969 0.0649618  0.06097288 0.13346926]\n",
      " [0.11834598 0.2386057  0.04991941 0.04318319 0.13403005 0.07190368\n",
      "  0.07101607 0.06798408 0.06163754 0.1433743 ]\n",
      " [0.12576723 0.17237545 0.03514131 0.04656377 0.1617204  0.07604362\n",
      "  0.06415148 0.07550685 0.06597449 0.17675539]]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
      "Loss:  1.7927319020609354\n"
     ]
    }
   ],
   "source": [
    "X = x_train_reshaped[:5]\n",
    "Y = y_train[:5]\n",
    "\n",
    "# train_step을 다섯 번 반복 돌립니다.\n",
    "for i in range(5):\n",
    "    W1, b1, W2, b2, _ = train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12667082, 0.16385654, 0.04583853, 0.04493287, 0.1472176 ,\n",
       "       0.13722601, 0.05640605, 0.05782246, 0.05952545, 0.16050365])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = x_train[:100] 에 대해 모델 추론을 시도합니다. \n",
    "X = x_train_reshaped[:100]\n",
    "Y = y_test[:100]\n",
    "result = predict(W1, b1, W2, b2, X)\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12667082 0.16385654 0.04583853 0.04493287 0.1472176  0.13722601\n",
      " 0.05640605 0.05782246 0.05952545 0.16050365]\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "0.14\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(W1, b1, W2, b2, X, Y)\n",
    "\n",
    "t = _change_one_hot_label(Y, 10)\n",
    "print(result[0])\n",
    "print(t[0])\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_params' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6163/1825141168.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0miter_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miters_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_params' is not defined"
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "    \n",
    "    W1, b1, W2, b2, Loss = train_step(x_batch, y_batch, W1, b1, W2, b2, learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b26eb2611282d90d9c5c5eb5497dd9fe754f45e0da87815f2566e1ffecfffe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
