{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.8579 - accuracy: 0.8011\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2493 - accuracy: 0.9297\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1885 - accuracy: 0.9457\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1588 - accuracy: 0.9555\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1337 - accuracy: 0.9616\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1157 - accuracy: 0.9678\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1005 - accuracy: 0.9709\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0906 - accuracy: 0.9747\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0815 - accuracy: 0.9774\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0756 - accuracy: 0.9793\n",
      "313/313 - 1s - loss: 0.1060 - accuracy: 0.9684\n",
      "test_loss: 0.10600697249174118 \n",
      "test_accuracy: 0.9684000015258789\n"
     ]
    }
   ],
   "source": [
    "# 평범한 tensorflow 활용한 mnist 코드\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# MNIST 데이터를 로드. 다운로드하지 않았다면 다운로드까지 자동으로 진행됩니다. \n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 딥러닝 모델 구성 - 2 Layer Perceptron\n",
    "model=keras.models.Sequential()\n",
    "model.add(keras.layers.Dense(50, activation='sigmoid', input_shape=(784,)))  # 입력층 d=784, 은닉층 레이어 H=50\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))   # 출력층 레이어 K=10\n",
    "model.summary()\n",
    "\n",
    "# 모델 구성과 학습\n",
    "model.compile(optimizer='adam',\n",
    "             loss='sparse_categorical_crossentropy',\n",
    "             metrics=['accuracy'])\n",
    "model.fit(x_train_reshaped, y_train, epochs=10)\n",
    "\n",
    "# 모델 테스트 결과\n",
    "test_loss, test_accuracy = model.evaluate(x_test_reshaped,y_test, verbose=2)\n",
    "print(\"test_loss: {} \".format(test_loss))\n",
    "print(\"test_accuracy: {}\".format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 39,760\n",
      "Trainable params: 39,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2개 이상의 레이어를 쌓아 만든 것을 다층 퍼셉트론 (Multio-Layer Perceptron; MLP) 더 깊어질 수록 deep 하다   \n",
    "보통 이것을 DNN 이라고 부른다   \n",
    "Fully-Connected Neural Network - 서로 다른 층에 위치한 노드와는 연결 관계가 없고, 인접한 층의 노드들만 연결   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(5, 784)\n"
     ]
    }
   ],
   "source": [
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X=x_train_reshaped[:5]\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 50)\n",
      "(50,)\n",
      "(5, 50)\n"
     ]
    }
   ],
   "source": [
    "WEIGT_INIT_STD = 0.1\n",
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 50\n",
    "\n",
    "# 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "# np.random.randn(m, n) : 평균 0, 표준편차 1의 가우시안 표준정규분포 난수를\n",
    "# matrix array(m, n) 생성\n",
    "W1 = WEIGT_INIT_STD * np.random.randn(INPUT_SIZE, HIDDEN_SIZE)  \n",
    "# bias 파라미터 b를 생성하고 Zero로 초기화\n",
    "b1 = np.zeros(HIDDEN_SIZE)\n",
    "\n",
    "a1 = np.dot(X, W1) + b1   # 은닉층 출력\n",
    "\n",
    "print(W1.shape)\n",
    "print(b1.shape)\n",
    "print(a1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.46526896,  0.3814905 , -0.65881936,  0.28439008,  0.64580803,\n",
       "        0.27773338, -0.13891993, -2.27023439,  0.46461251,  0.20724447,\n",
       "       -0.6600686 , -1.11689013, -0.72708612,  0.58872886,  0.17576494,\n",
       "       -2.01319246,  0.21720359,  0.04774898, -2.3360085 , -0.23473761,\n",
       "       -0.6448875 ,  0.70606638, -0.28331868,  1.0130121 , -1.01960786,\n",
       "       -0.63295971, -0.27268662, -0.2950952 ,  0.71301173, -0.91111904,\n",
       "        0.08378465,  0.34860133, -0.26997828, -0.9095872 , -0.60740233,\n",
       "       -1.11600917,  1.50846132, -0.42726893,  0.98494027, -0.38764511,\n",
       "        0.25065557,  0.2683503 ,  0.15854438,  0.49911732, -0.26336609,\n",
       "        1.07506732,  0.79687649, -0.41325856, -0.11274781,  0.95011104])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 첫 번째 데이터의 은닉층 출력을 확인해보자. 50 dim\n",
    "a1[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 활성화 함수 (Activation Functions)\n",
    "nonlinear한 출력으로 만들어줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38573663 0.59423254 0.34100488 0.57062218 0.65606519 0.56899044\n",
      " 0.46532576 0.09361832 0.61410782 0.55162647 0.3407242  0.24658859\n",
      " 0.32583448 0.64307343 0.54382846 0.11782474 0.55408842 0.51193498\n",
      " 0.08818434 0.44158359 0.34414255 0.66953139 0.42964035 0.73360921\n",
      " 0.26510379 0.34683973 0.43224765 0.42675694 0.6710663  0.2867709\n",
      " 0.52093392 0.58627836 0.43291243 0.28708432 0.35265199 0.24675229\n",
      " 0.81883306 0.39477867 0.72808737 0.40428432 0.56233785 0.56668786\n",
      " 0.53955328 0.62225188 0.43453643 0.74555938 0.68930594 0.39813104\n",
      " 0.47184287 0.72113751]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "z1 = sigmoid(a1)\n",
    "print(z1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://reniew.github.io/12/\n",
    "https://pozalabs.github.io/Activation_Function/\n",
    "\n",
    "1. 하이퍼볼릭 탄젠트 (tanh)\n",
    "2. ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단일 레이어 구현 함수\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01870923  0.251838   -0.3295376   0.5905999  -0.21985127 -0.25263557\n",
      "  0.47613115 -0.28476619 -0.53146476  0.653538  ]\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 784\n",
    "HIDDEN_SIZE = 50\n",
    "OUTPUT_SIZE = 10\n",
    "\n",
    "W1 = WEIGT_INIT_STD * np.random.randn(INPUT_SIZE, HIDDEN_SIZE)\n",
    "b1 = np.zeros(HIDDEN_SIZE)\n",
    "W2 = WEIGT_INIT_STD * np.random.randn(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "b2 = np.zeros(OUTPUT_SIZE)\n",
    "\n",
    "a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "z1 = sigmoid(a1)\n",
    "a2, cache2 = affine_layer_forward(z1, W2, b2)    # z1이 다시 두번째 레이어의 입력이 됩니다. \n",
    "\n",
    "print(a2[0])  # 최종 출력이 OUTPUT_SIZE만큼의 벡터가 되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08729045, 0.11440998, 0.0639699 , 0.16054114, 0.0713858 ,\n",
       "       0.06908342, 0.14317698, 0.06689901, 0.05227329, 0.17097003])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = softmax(a2)\n",
    "y_hat[0]  # 10개의 숫자 중 하나일 확률이 되었습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수\n",
    "https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718   \n",
    "Loss / Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정답 - 원 핫 인코딩\n",
    "# 정답 라벨을 One-hot 인코딩하는 함수\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T\n",
    "\n",
    "Y_digit = y_train[:5]\n",
    "t = _change_one_hot_label(Y_digit, 10)\n",
    "t     # 정답 라벨의 One-hot 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08729045 0.11440998 0.0639699  0.16054114 0.0713858  0.06908342\n",
      " 0.14317698 0.06689901 0.05227329 0.17097003]\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hat[0])\n",
    "print(t[0])\n",
    "# 아직은 정답과는 멀다. 대부분 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3619536915228356\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "Loss = cross_entropy_error(y_hat, t)\n",
    "print(Loss)\n",
    "# 이제 이 오차를 줄여보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 경사하강법\n",
    "Gradient Descent   \n",
    "\n",
    "https://aileen93.tistory.com/71   \n",
    "https://reniew.github.io/13/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01745809,  0.022882  ,  0.01279398,  0.03210823,  0.01427716,\n",
       "        -0.18618332,  0.0286354 ,  0.0133798 ,  0.01045466,  0.03419401],\n",
       "       [-0.18251002,  0.02338945,  0.01694397,  0.0378616 ,  0.01654318,\n",
       "         0.01385245,  0.02254635,  0.01490286,  0.01157563,  0.02489453],\n",
       "       [ 0.01847007,  0.02249367,  0.01571524,  0.03381219, -0.18599181,\n",
       "         0.01288507,  0.02332462,  0.01477249,  0.01040825,  0.03411021],\n",
       "       [ 0.01690509, -0.1776809 ,  0.01514527,  0.03383641,  0.01685448,\n",
       "         0.01312581,  0.02440253,  0.01235004,  0.01135418,  0.03370709],\n",
       "       [ 0.01636511,  0.02400788,  0.0164657 ,  0.03113842,  0.01711983,\n",
       "         0.01245757,  0.02708265,  0.01413112,  0.00975563, -0.1685239 ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_num = y_hat.shape[0]\n",
    "dy = (y_hat - t) / batch_num\n",
    "dy    # softmax값의 출력으로 Loss를 미분한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7b26eb2611282d90d9c5c5eb5497dd9fe754f45e0da87815f2566e1ffecfffe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py37': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
